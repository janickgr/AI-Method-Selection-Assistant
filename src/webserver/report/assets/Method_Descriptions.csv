ID;Name;Beschreibung;Vorteile;Nachteile;Aufgabentypen
1;Neural Networks and Deep Learning;Um die große Generalisierungskapazität und Leistung von DNNs richtig ausnutzen zu können, ist eine große Datenmenge erforderlich. Es wird auch Zeit benötigt, sowohl für die Optimierung der Architektur und der Hyperparameter als auch für das Training der Modelle. Außerdem sollte die Erklärbarkeit des Modells aufgrund seiner Komplexität kein primärer Faktor bei seiner Verwendung sein. Schließlich erfordern diese Algorithmen eine große Rechenleistung, um zu trainieren.;Diese Art von Algorithmus zeichnet sich in Bereichen aus, in denen die Daten komplex sind und das Feature Engineering schwierig ist. Sie sind in der Lage, hochkomplexe Funktionen zu modellieren, in Bereichen, in denen andere ML-Algorithmen nur schwer gute Ergebnisse liefern können. Darüber hinaus haben die DNNs eine große Fähigkeit ihre Leistung mit einer zunehmenden Da-tenmenge weiter zu verbessern.;Anwendungen, denen große Mengen an Trainingsdaten oder eine hohe Rechenleistung fehlen oder bei denen die Interpretierbarkeit eines gelernten Modells wichtig ist, sind für DNN nicht optimal. Darüber hinaus können DNNs eine schlechte Alternative für Probleme sein, bei denen die Zeit für das Training des Modells und die Optimierung seiner Architektur und Hyperparameter kurz ist.;Zeichnet eine gute Leistung bei strukturierten Daten auf, aber zeichnet sich besonders auf bei Auf-gaben mit komplexen unstrukturierten Daten. Die Anwendungen von DNNs sind sehr vielfältig, z. B. in Suchmaschinen, autonomen Autos, Hirnschlagvorhersage, Sentiment-Analyse u. a.
2;RNN;Es hat ähnliche Anforderungen wie DNNs, wie z. B. hohe Rechenleistung, Bedarf an großen Datenmengen, Zeit zum Trainieren und Optimieren der Modelle. Da es sich jedoch um eine konnektionistische Antwort für sequenzielle Daten handelt, konzentriert es sich auf diese Art von Daten, wie Text, Audio und Zeitreihen. Sie sollte auch in Fällen angewendet werden, in denen die Erklärbarkeit nicht entscheidend ist. Seine Leistung bei unstrukturierten sequentiellen Daten hebt sich hervor.;Wie DNNs kann es große Datenmengen nutzen und komplexe Funktionen erlernen. Sie kann das Problem der explodierenden oder verschwindenden Gradienten umgehen, was sie besonders für Datenfolgen geeignet macht.;Es hat auch ähnliche Nachteile wie DNNs, wie z. B. geringe Erklärbarkeit, Bedarf an hoher Rechenleistung und große Datenmengen. Darüber hinaus macht seine rekursive Struktur das Lernen tendenziell langsamer als andere konnektionistische Algorithmen.;Rekurrente Neuronale Netze (RNN) dominieren die Verarbeitung von sequenziellen Daten wie Text und Sprache. Andere Anwendungen für diese Algorithmen sind Vorhersagen, z. B. für den Energiebedarf und die Preise von Vermögenswerten auf dem Finanzmarkt.
3;CNN;Es teilt auch viele Eigenschaften mit DNNs und RNNs, wie z. B., dass es große Datenmengen benötigt, um sein volles Potenzial zu entfalten. Es konzentriert sich auf sehr hochdimensionale Aufgaben, wie z. B. Bildanalyse und Computer Vision, besonders bei unstrukturierten Daten. Sie sollte auch dort eingesetzt werden, wo die Erklärbarkeit nicht im Mittelpunkt des Problems steht und hohe Rechenleistung beansprucht.;Tendenziell bietet dieser Algorithmus eine höhere Trainingsgeschwindigkeit als andere konnektionistische Algorithmen, vor allem aufgrund der gemeinsamen Nutzung von Parametern. Können mit Problemen hoher Dimensionalität umgehen und sind in der Lage, sehr komplexe Funktionen zu lernen. Sie können große Datenmengen effektiv nutzen, einschließlich Data Augmentation.;Sie benötigen große Datenmengen, um eine gute Leistung zu erbringen, zusätzlich zu einer großen Rechenkapazität. Sie weisen eine geringe Erklärbarkeit und hohe Komplexität auf.;Konvolutionelle Neuronale Netze (Convolutional Neural Networks, oder CNN) haben sich zum Standard für die Bildverarbeitung etabliert. Probleme wie Stilübertragung, Bilderkennung und Computer Vision sind einige Beispiele für Bereiche, in denen sich CNNs besonders hervorgetan haben. Seine Leistung zeichnet sich besonders bei unstrukturierten Daten auf.
4;Bayesian Methods;Es benötigt keine großen Datenmengen und ist in Situationen, in denen die Lerngeschwindigkeit entscheidend ist, sehr akzeptiert. Für eine zufriedenstellende Leistung muss die Problemkomplexität gering sein.;Es hat eine hohe Trainingsgeschwindigkeit ist ein Algorithmus sehr leicht zu verstehen und ein-fach zu implementieren. Außerdem Naive Bayes ist ein sehr flexibler Algorithmus, der auf Probleme verschiedener Kontexte und Eigenschaften angewendet werden kann.;Bei mehreren Problemen zeigt er eine schlechtere Leistung als andere komplexere Algorithmen. Es stellt eine Schwierigkeit der Generalisierung auf Probleme höherer Komplexität dar und seine Leistung hängt stark von der Einfachheit der Daten und Probleme ab.;Es kann für verschiedene Arten von Problemen verwendet werden, z. B. für Spam-Filter, Vorhersagen und andere. Aufgrund seiner Leistungsfähigkeit und Flexibilität ist er einer der meistgenutzten Algorithmen in Unternehmen wie Google.
5;"Tree-Based Methods";Er ist ein sehr vielseitiger Algorithmus, der in einer Vielzahl von Kontexten verwendet werden kann. Die Daten sollten möglichst sauber und rauschfrei sein, da die Tendenz zum Overfitting besteht. Seine Performance bei strukturierten hebt sich vor. Seine Verwendung wird oft mit Situationen in Verbindung gebracht, in denen die Erklärbarkeit entscheidend ist.;Der Algorithmus Decision Trees zeichnet sich durch hohe Erklärbarkeit und hohe Trainingsgeschwindigkeit aus. Er hat die Fähigkeit, komplexe Funktionen zu modellieren und kann sehr gut mit hoher Dimensionalität und dem Vorhandensein von unwichtigen Features umgehen. Sie zeigen auch bei unausgewogenen Klassen eine zufriedenstellende Leistung.;Er hat eine Tendenz zum Overfitting, außerdem hängt seine Erklärbarkeit direkt mit der Tiefe und der Anzahl der Knoten des Baums zusammen, die bei komplexeren Problemen tendenziell hoch sind und seine Erklärbarkeit beeinträchtigen.;Er ist einer der am häufigsten verwendeten ML-Algorithmen. Die Anwendungsfälle reichen von der Vorhersage von Gerichtsentscheidungen bis zur Verfolgung von Bewegungen. Durch seine Vielseitigkeit kann er in unzähligen Fällen zufriedenstellend eingesetzt werden, besonders bei strukturierten Daten.
6;Bagging;Der Bagging Algorithmus benötigt mehr Rechenleistung als Entscheidungsbäume. Er ist ein sehr vielseitiger Algorithmus auch bei komplexeren und verteilten Daten, der sich besoders bei strukturierten Daten auszeichnet. Für diese Arbeit wird die Implementierung von Bagging auf Basis von Entscheidungsbäumen in Betracht gezogen, da sie die in der Literatur und in Anwendungsfällen am häufigsten vorkommende Implementierung ist.;Es weist Eigenschaften auf, die den Entscheidungsbäumen gemeinsam sind, wie z. B. die Fähigkeit, komplexe Funktionen zu modellieren, der einfache Umgang mit unausgeglichenen Daten, hohe Dimensionalität und Features mit geringer Wichtigkeit. Durch die Kombination mehrerer Bäume in seiner Implementierung ist er resistenter gegen Overfitting und weist tendenziell eine höhere Generalisierungskapazität auf.;Aufgrund der Tatsache, dass es sich um ein Ensemble handelt, weist es höhere Rechenkosten und eine geringere Trainingsgeschwindigkeit als Entscheidungsbäume auf. Ein wichtiger Punkt ist der Verlust an Transparenz und Erklärbarkeit im Vergleich zu Entscheidungsbäumen durch die Kom-bination mehrerer Modelle.;Sie gilt als eine allgemeine Technik, die für die große Mehrheit der Probleme anwendbar ist. Sie kann in vielen Fällen die Varianz verringern, ohne die Verzerrung wesentlich zu erhöhen.
7;Random Forests;Der Random Forest Algorithmus hat sehr ähnliche Eigenschaften wie Bagging. Es ist sehr vielseitig und zeichnet sich besonders bei komplexen strukturierten Daten aus, bei denen das Training schnell erfolgen muss.;Es ist ein Algorithmus, der in vielen verschiedenen Kontexten und Situationen eine gute Leistung zeigt. Er ist in der Lage gute Ausnutzung aus strukturierten Datenvolumen zu erzielen. Seine Implementierung ist passend zu paralleler Datenverarbeitung. Seine Generalisierungsfähigkeit deckt einfache und komplexe Daten ab und ist erheblich resistent gegen Overfitting. Es zeigt außergewöhnliche Ergebnisse bereits mit Standardparametern und ist in vielen Fällen eine gute Alternative, um in einem ML-Projekt schnell Mehrwert zu generieren.;Da es sich um ein Ensemble aus zahlreichen Bäumen handelt, verliert es teilweise seine Erklärbarkeit. Da er bereits mit Default-Parametern eine hervorragende Performance aufweist, bringt die Aufgabe der Parameteroptimierung wenigen Vorsprung.;Es kann für viele verschiedene Aktivitäten verwendet werden, unter anderem für die Kreditanalyse. es eignet sich für die Verwendung mit strukturierten Daten, bei denen Sie schnell eine Leistungsbasislinie ermitteln möchten.
8;Gradient Boosting;Die Anwendung des Gradient-Boosting-Algorithmus erfordert mehr Rechenleistung als die Verwendung von Random Forests und Bagging. Sie arbeiten besser mit großen strukturierten Datenmengen, um ihre immense Generalisierungsfähigkeit auszunutzen.;Es hat große Einfachheit beim Lernen komplexer Funktionen, kann gut mit unausgewogenen Daten und hoher Dimensionalität umgehen. Es hat eine ausgezeichnete Leistung mit strukturierten Daten. Präsentiert eine gute Trainingsgeschwindigkeit, besonders bei den neuesten Implementie-rungen.;Kann eine Overfitting aufweisen, wenn nicht richtig regularisiert wird. Er benötigt in der Regel mehr Rechenkapazität als andere Ensemble-Algorithmen und hat tendenziell einen größeren Bedarf an Hyperparameter-Optimierung als andere Ensembles.;"Er ist einer der siegreichsten Algorithmen in ML-Wettbewerben wie dem Kaggle-Portal, besonders wenn es um strukturierte Daten geht. Sowohl bei Vorhersage- und Prognoseaufgaben als auch
bei den unterschiedlichsten Aufgaben, von der Preisvorhersage bis zur Kreditanalyse, zeigt Gradient Boosting außergewöhnliche Leistungen."
9;K-Nearest Neighbors ;Er ist tendenziell effektiver bei Problemen mit geringer Dimensionalität. Die Größe des Datensatzes sollte aus Gründen des Rechenaufwands nicht zu groß sein. Sie kann eine gute Alternative für Probleme sein, die Einfachheit und Erklärbarkeit erfordern.;"KNN ist ein einfacher Algorithmus mit guter Erklärbarkeit. Es ist in der Lage, insbesondere bei Problemen mit geringer Dimensionalität eine gute Leistung zu präsentieren. Durch das ""faule Lernen"" benötigt es keine Einarbeitungszeit. Aufgrund der geringen Komplexität ist tendenziell nur wenig Hyperparameter-Tuning erforderlich.";Da es auf Distanz zur Darstellung der Ähnlichkeit basiert, ist es besonders anfällig für Leistungsverluste bei Problemen mit hoher Dimensionalität. Obwohl es keine Trainingszeit benötigt, kann seine Ausführungszeit bei großen Datensätzen übermäßig hoch sein. Für komplexere Probleme kann es eine unbefriedigende Generalisierungskapazität aufweisen.;Sie werden in verschiedenen Kontexten und Problemstellungen eingesetzt, wie z. B. bei der Erkennung handschriftlicher Ziffern, sowie bei der Erstellung von Empfehlungssystemen.
10;Support Vector Machines;Der SVM-Algorithmus ist in der Lage, eine hohe Komplexität von Daten und Problemen gut zu bewältigen. Allerdings sollte die Datenmenge wegen der Rechenkosten und der Trainingsgeschwindigkeit nicht zu groß sein.;Er verfügt über eine große Generalisierungskapazität und ist in der Lage, komplexe Funktionen zu modellieren. SVMs haben nur einen optimalen Punkt für die Parameter, was ihr Lernen erleichtert und sie zuverlässiger macht. Sie weisen eine gute Beständigkeit gegen Overfitting auf. Tendenziell sind sie beständiger gegen Leistungsverluste bei Problemen mit hoher Dimensionalität als andere Algorithmen.;Aufgrund seiner hohen Komplexität müssen seine Hyperparameter mit Sorgfalt abgestimmt werden. Seine Erklärbarkeit ist reduziert. Die Trainingsgeschwindigkeit ist für große Datensätze tendenziell unpraktikabel, was seine Anwendbarkeit erheblich einschränkt. Seine Rechenkosten wachsen überlinear mit dem Wachstum des Datensatzes, was seine Skalierbarkeit in vielen Fällen kritisch macht.;Dies waren einige der am häufigsten verwendeten Algorithmen bis zur Rückkehr der Konnektionisten mit dem Aufkommen des Deep Learning. Sie werden weiterhin bei vielen komplexen Problemen eingesetzt, z. B. bei der Energiebedarfsprognose.
11;"Linear Models";Lineare und logistische Regressionen sollten vorzugsweise bei Problemen angewendet werden, bei denen die Erklärbarkeit tendenziell wichtiger ist als die Leistung. Fälle, bei denen sowohl die Menge der verfügbaren Daten als auch die Komplexität des Problems gering sind, können ebenfalls für den Einsatz dieser Algorithmen geeignet sein.;Sie haben eine hohe Erklärbarkeit, sodass sie häufig für Studien von Daten und den Phänomenen, die sie darstellen, verwendet werden. Sie weisen eine sehr reduzierte Komplexität auf, was ihre Modellierung schnell macht. Ihre Trainingszeit ist gering und sie zeigen eine gute Leistung bei Daten mit geringer Dimensionalität und Komplexität.;Aufgrund der geringen Komplexität weisen logistische und lineare Regressionen eine geringe Generalisierungsfähigkeit auf, insbesondere bei komplexeren Problemen. Außerdem ist er tendenziell nicht in der Lage, seine Leistung bei der Verwendung großer Datenmengen zu steigern wie andere komplexeren Algorithmen aufgrund seiner Neigung zur Unteranpassung.;Sie wird immer noch in vielen Fällen verwendet, vor allem wegen ihrer Erklärbarkeit und Nachvollziehbarkeit. Beispiele von seiner Verwendung sind experimentelle wissenschaftliche Studien, die Vorhersage der Ausbreitung epidemischer Krankheiten und die Analyse von Volkszählungsdaten.
12;K-Means;Die Verwendung des K-Means-Algorithmus erfordert eine konvexe Verteilung und ausgewogene Klassen in den Daten, um eine gute Performance zu gewährleisten. Jeder Cluster ist ungefähr ein kugelförmiger Globus im Hyperraum, die Globus sollen weit voneinander entfernt sein, und sie sollen alle ein ähnliches Volumen haben und sollen eine ähnliche Anzahl von Elementen enthalten.;Der K-Means-Algorithmus ist tendenziell effektiv beim Clustering großer Datensätze und hat niedrige Rechenkosten und hohe Skalierbarkeit, was ihn für Big-Data-Aufgaben geeignet macht. Er kann seine Leistung bei größeren Datenmengen erheblich steigern.;Die Anzahl der Cluster muss im Voraus festgelegt werden. Die Verwendung ist auf eine bestimmte Datenkomplexität beschränkt. Sie kann generell mit komplexeren Datenverteilungen und mit unausgewogenen Daten nicht angemessen umgehen.;Zusätzlich zu den geringen Rechenkosten kann der K-Means-Algorithmus in vielen praktischen Situationen und Big Data Aufgaben gute Ergebnisse liefern, z. B. bei der Erkennung von Anomalien und der Segmentierung von Daten.
13;Hierarchical Clustering ;Die Datenmenge sollte klein oder mittelgroß sein, die gewünschte Anzahl von Clustern sollte be- vorzugt höher sein und es ist interessant, dass Themen wie Erklärbarkeit und Verständlichkeit bei der Anwendung dieses Algorithmus grundlegend wichtig sind.;Es bietet gute Vorteile hinsichtlich der Erklärbarkeit und der Flexibilität bei der Definition der Anzahl der Cluster. Es ermöglicht eine Merkmalsähnlichkeitsanalyse und zeigt gute Ergebnisse für kleinere Datensätze und mit einer größeren Anzahl von Clustern. Außerdem ist es ein Algo- rithmus, der resistent gegen Rauschen in den Daten ist.;hohe Rechenkosten und benötigt eine hohe Trainingszeit, wodurch es für Big-Data-Probleme nicht sehr geeignet ist. Bei größeren Datensätzen werden tendenziell kein Leistungsgewinne verzeichnet.;Aufgrund des hohen Rechenaufwands sollte die Datenmenge nicht zu groß sein. Sie sollte vorzugsweise in Probeläufen mit einer größeren Anzahl von Clustern und in Fällen verwendet werden, in denen Erklärbarkeit und Flexibilität bei der Wahl der Anzahl von Clustern wichtig ist.
14;Expectation Maximization ;Große Datenmengen und Bedarf an hoher Leistung. Geräuschlose und ausgewogene Daten sind besonders erwünscht. Seine starke statistische Basis verleiht ihm eine große Flexibilität in der Anwendbarkeit.;Dieser Algorithmus kann ohne hohen Rechenaufwand und mit geringer Trainingszeit hervorragende Ergebnisse liefern. Kommt gut mit hoher Dimensionalität zurecht und kann seine Leistung bei größeren Datenmengen deutlich verbessern.;Bei einer sehr geringen Anzahl von Objekten hat er Schwierigkeiten, Cluster zu finden .Er Tendiert erheblich vom Rauschen in den Daten, beeinflusst zu werden und er könnte mehr Anpassung an die Hyperparameter erfordern als andere Algorithmen.;Probleme vorzugsweise mit großen Datenmengen, mit ausgewogenen Daten und ohne viel Rau- schen. Insbesondere in Fällen, bei denen Leistung, Skalierbarkeit und Trainingsgeschwindigkeit wichtig sind und die Notwendigkeit der Optimierung von Hyperparametern nicht hinderlich ist.
15;DBSCAN;Es benötigt keine Anzahl von Clustern als Eingabe, sondern den Radius der Nachbarschaft und die Mindestanzahl von akzeptablen Punkten in einem Cluster. Komplexe Datenverteilungen werden akzeptiert, ebenso wie große Datenmengen. Es geht von der Annahme aus, dass Daten aus dem gleichen Cluster in einer Region des Hyperraums gruppiert sind.;Er kann mit hochkomplexen Verteilungen umgehen, z. B. S-förmig. Er ist ideal für die Verarbei- tung räumlicher Daten. Er kann große Datenmengen gut verarbeiten und ist sehr resistent gegen Rauschen in den Daten. Er benötigt keine Anzahl von Clustern als Eingabe und kann bei unterschiedlichen Hyperparameter-Einstellungen große Unterschiede in den Ergebnissen zeigen.;Obwohl die Anzahl der Cluster nicht erforderlich ist, kann es sehr schwierig sein, Werte für den Nachbarschaftsradius und die Mindestanzahl akzeptabler Punkte für einen Cluster zu finden.;Es ist ideal für die Anwendung auf räumliche Probleme mit komplexen Datenverteilungen. Es ist in der Lage, Big-Data-Probleme zu lösen und weist eine gute Flexibilität und Skalierbarkeit auf. Es ist möglicherweise keine gute Option für Probleme, bei denen die Anzahl der Cluster definiert ist oder wenn ein Hyperparameter-Abstimmungsprozess nicht gewünscht ist.
16;PCA;Es handelt sich um eine lineare Transformationstechnik, die vor allem bei linear korrelierten Daten anwendbar ist. Es ist ein einfacher und leichtgewichtiger Algorithmus, der nicht viele Rechenressourcen und Trainingszeit benötigt.;Da es leichtgewichtig ist, schnell trainiert und keine Abstimmung der Hyperparameter erfordert, kann es in kurzer Zeit hervorragende Ergebnisse liefern. Es ist eines der Hauptwerkzeuge eines Data Scientist, sowohl um Daten für das Training vorzubereiten, als auch um multidimensionale Datenvisualisierung zu ermöglichen und damit Erkenntnisse zu gewinnen.;Es kann mit möglichen Nichtlinearitäten der Daten nicht gut umgehen. Manchmal sind sie nicht in der Lage, höhere Ebenen der Datenkomplexität zu erklären.;Datenaufbereitung und Visualisierung von mehrdimensionalen Daten. Da es leichtgewichtig und schnell ist, kann es für das Prototyping in großen Datensätzen zur Hypothesenvalidierung verwendet werden, bevor die Notwendigkeit der Verwendung komplexerer Algorithmen in Betracht gezogen wird. Mit der Erhöhung der Projektionsdimensionen können Ergebnisse im Vergleich zum Autoencoder präsentiert werden.
17;ISOMAP ;Anstatt die euklidische Distanz zu berücksichtigen, wie bei der PCA, wird bei ISOMAP die geo- dätische Distanz betrachtet, was diesem Algorithmus die Fähigkeit verleiht, komplexere nichtli- neare Strukturen in den Daten zu untersuchen. Diese Verarbeitung ist jedoch sehr rechenintensiv, so dass ihre Anwendung in Bezug auf das Datenvolumen sehr begrenzt ist, obwohl sie mit kom- plexeren Daten umgehen kann.;Er kann auf komplexere Probleme und Daten angewendet werden, da sie besser mit der Nichtlinearität der Daten umgehen kann. Er ist relativ einfach zu implementieren und erfordert wenig Optimierung von Hyperparametern, insbesondere im Vergleich zur Komplexität von Autoencodern.;Bei vielen Problemen liefert sie nur wenig bessere Ergebnisse als die PCA, bei der Verarbeitung großer Datenmengen hat sie eine sehr große Einschränkung, die ihren Einsatz begrenzt.;Ideal für den Einsatz in der Dimensionalitätsreduktion bei komplexeren Daten und bei reduzierter Datenmenge. Stellt eine Zwischenstufe zwischen PCA und Autoencodern dar.
18;Autoencoding NN;Er teilt die Anforderungen mit anderen konnektionistischen Algorithmen und benötigt große Datenmengen, Rechenleistung und Trainingszeit, um eingesetzt werden zu können. Außerdem erfordert es eine hohe Expertise des Teams und viel Aufwand für die Abstimmung der Hyperparameter.;Er hat den Vorteil, dass komplexe Strukturen erlernt und große Datenmengen ausgenutzt werden können. Bessere Leistung bei geringerer Anzahl an Dimensionen bei der Projektion. Seine Verwendung zur visuellen Darstellung von Multidimensionen tendiert dazu, Bilder auf Ecken und Kanten zu projizieren, was zu Interpretationsvorteilen führen kann.;Er stellt eine hohe Entwicklungskomplexität dar, mit vielen architektonischen Elementen und Hyperparametern, die optimiert werden müssen. Außerdem benötigt es eine große Rechenkapazität, große Datenmengen und eine lange Zeit, um trainiert zu werden. Außerdem kann es in einem lokalen Minimum stecken bleiben und aufgrund der zufälligen Initialisierung ist es keine deterministische Methode. ;Wird in Situationen verwendet, in denen große Datenmengen zur Lösung komplexer Probleme gelernt werden müssen. Dazu gehören Datenkompression, Visualisierung komplexer mehrdimensionaler Daten, Situationen, in denen es wiederkehrende Strukturen in den Daten gibt.